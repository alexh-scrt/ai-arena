# üéº PHASE 2: CONTINUED (2.6-2.10)

## Phase 2.6: Progression Controller (from AI-Talks)

### Task 2.6.1: Create Arena Progression Controller

**File**: `src/arena/orchestration/controllers/progression_controller.py`

**Instructions for Claude Code**:

This adapts AI-Talks ProgressionController to detect orbiting and quality issues.

**Reference**: `talks/src/controllers/progression_controller.py` (CRITICAL - study entire file)

```python
"""
Progression Controller for Arena competitions.

Adapted from AI-Talks ProgressionController with Arena extensions.

Key responsibilities (from AI-Talks):
- Detect "orbiting" (circular discussions)
- Monitor discussion depth vs breadth
- Identify stagnation
- Recommend interventions

Arena extensions:
- Track per-character novelty trends
- Detect paraphrasing early
- Monitor competitive dynamics
- Integration with anti-gaming systems
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import logging

from ..states.competition_state import CompetitionState, CompetitionExchange
from ..talks_loader import get_talks_loader

logger = logging.getLogger(__name__)


@dataclass
class ProgressionConfig:
    """
    Configuration for progression analysis (from AI-Talks).
    """
    # Orbiting detection (from AI-Talks)
    orbiting_window: int = 5  # Number of turns to analyze
    orbiting_threshold: float = 0.75  # Semantic similarity threshold
    
    # Stagnation detection (from AI-Talks)
    stagnation_window: int = 8  # Turns without progress
    novelty_threshold: float = 0.3  # Minimum novelty to avoid stagnation
    
    # Arena-specific
    per_character_novelty_tracking: bool = True
    paraphrasing_sensitivity: float = 0.85  # Higher = more sensitive


@dataclass
class ProgressionAnalysis:
    """
    Results of progression analysis.
    """
    # Orbiting (from AI-Talks)
    is_orbiting: bool
    orbiting_score: float  # 0-1, higher = more circular
    
    # Stagnation (from AI-Talks)
    is_stagnant: bool
    stagnation_turns: int
    
    # Quality (from AI-Talks)
    average_novelty: float
    depth_progress: float
    
    # Arena-specific
    per_character_novelty: Dict[str, float]  # character_id -> avg novelty
    declining_participants: List[str]  # character_ids with declining novelty
    
    # Recommendations (from AI-Talks + Arena)
    should_intervene: bool
    intervention_type: Optional[str]  # "narrator_prompt", "force_pivot", None
    recommendation: str


class ArenaProgressionController:
    """
    Monitors and analyzes competition progression.
    
    Based on AI-Talks ProgressionController with Arena competitive extensions.
    
    Uses semantic similarity to detect:
    - Orbiting (circular discussions)
    - Stagnation (lack of progress)
    - Declining novelty per character
    - Paraphrasing patterns
    """
    
    def __init__(
        self,
        config: Optional[ProgressionConfig] = None,
        embedding_model: Optional[Any] = None
    ):
        """
        Initialize progression controller.
        
        Args:
            config: Configuration for analysis
            embedding_model: Model for semantic similarity
                           (from AI-Talks, typically sentence-transformers)
        """
        self.config = config or ProgressionConfig()
        
        # Load AI-Talks progression controller as base
        talks_loader = get_talks_loader()
        talks_config = {
            'orbiting_window': self.config.orbiting_window,
            'orbiting_threshold': self.config.orbiting_threshold,
        }
        self.talks_controller = talks_loader.create_progression_controller(
            config=talks_config
        )
        
        # Embedding model for semantic similarity
        self.embedding_model = embedding_model
        if self.embedding_model is None:
            self._initialize_embedding_model()
        
        # Track embeddings for efficiency
        self.exchange_embeddings: Dict[int, np.ndarray] = {}
        
        logger.info("ArenaProgressionController initialized")
    
    def _initialize_embedding_model(self):
        """Initialize sentence embedding model (from AI-Talks pattern)"""
        try:
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer(
                'all-MiniLM-L6-v2'  # AI-Talks standard
            )
            logger.info("Loaded embedding model: all-MiniLM-L6-v2")
        except ImportError:
            logger.warning(
                "sentence-transformers not available. "
                "Progression analysis will be limited."
            )
            self.embedding_model = None
    
    def analyze_progression(
        self,
        competition_state: CompetitionState
    ) -> ProgressionAnalysis:
        """
        Analyze competition progression (main method).
        
        Args:
            competition_state: Current state
        
        Returns:
            Analysis with recommendations
        """
        if not competition_state.exchanges:
            # No exchanges yet
            return ProgressionAnalysis(
                is_orbiting=False,
                orbiting_score=0.0,
                is_stagnant=False,
                stagnation_turns=0,
                average_novelty=1.0,
                depth_progress=0.0,
                per_character_novelty={},
                declining_participants=[],
                should_intervene=False,
                intervention_type=None,
                recommendation="Competition just started."
            )
        
        # Detect orbiting (from AI-Talks)
        orbiting_result = self._detect_orbiting(competition_state)
        
        # Detect stagnation (from AI-Talks)
        stagnation_result = self._detect_stagnation(competition_state)
        
        # Analyze novelty (from AI-Talks + Arena)
        novelty_result = self._analyze_novelty(competition_state)
        
        # Per-character analysis (Arena-specific)
        character_novelty = self._analyze_per_character_novelty(
            competition_state
        )
        
        # Determine if intervention needed
        should_intervene, intervention_type = self._should_intervene(
            orbiting_result,
            stagnation_result,
            novelty_result
        )
        
        # Generate recommendation
        recommendation = self._generate_recommendation(
            orbiting_result,
            stagnation_result,
            novelty_result,
            character_novelty
        )
        
        return ProgressionAnalysis(
            is_orbiting=orbiting_result['is_orbiting'],
            orbiting_score=orbiting_result['score'],
            is_stagnant=stagnation_result['is_stagnant'],
            stagnation_turns=stagnation_result['turns'],
            average_novelty=novelty_result['average'],
            depth_progress=0.0,  # TODO: Implement depth tracking
            per_character_novelty=character_novelty['averages'],
            declining_participants=character_novelty['declining'],
            should_intervene=should_intervene,
            intervention_type=intervention_type,
            recommendation=recommendation
        )
    
    def _detect_orbiting(
        self,
        competition_state: CompetitionState
    ) -> Dict:
        """
        Detect if discussion is orbiting (from AI-Talks).
        
        Orbiting = conversation going in circles, repeating same ideas.
        
        Uses semantic similarity across recent exchanges.
        """
        window = self.config.orbiting_window
        threshold = self.config.orbiting_threshold
        
        # Get recent exchanges
        recent = competition_state.exchanges[-window:] if len(
            competition_state.exchanges) >= window else competition_state.exchanges
        
        if len(recent) < 3:
            return {'is_orbiting': False, 'score': 0.0}
        
        # Compute embeddings if not cached
        for exchange in recent:
            if exchange.turn_number not in self.exchange_embeddings:
                embedding = self._get_embedding(exchange.content)
                self.exchange_embeddings[exchange.turn_number] = embedding
        
        # Calculate pairwise similarities
        similarities = []
        for i, ex1 in enumerate(recent):
            for ex2 in recent[i+1:]:
                emb1 = self.exchange_embeddings[ex1.turn_number]
                emb2 = self.exchange_embeddings[ex2.turn_number]
                
                # Cosine similarity
                similarity = self._cosine_similarity(emb1, emb2)
                similarities.append(similarity)
        
        if not similarities:
            return {'is_orbiting': False, 'score': 0.0}
        
        # Average similarity
        avg_similarity = np.mean(similarities)
        
        # Orbiting if high similarity across recent exchanges
        is_orbiting = avg_similarity > threshold
        
        if is_orbiting:
            logger.warning(
                f"Orbiting detected! Average similarity: {avg_similarity:.2f}"
            )
        
        return {
            'is_orbiting': is_orbiting,
            'score': float(avg_similarity)
        }
    
    def _detect_stagnation(
        self,
        competition_state: CompetitionState
    ) -> Dict:
        """
        Detect if discussion is stagnant (from AI-Talks).
        
        Stagnation = low novelty over multiple turns.
        """
        window = self.config.stagnation_window
        threshold = self.config.novelty_threshold
        
        # Get recent exchanges with novelty scores
        recent = competition_state.exchanges[-window:] if len(
            competition_state.exchanges) >= window else competition_state.exchanges
        
        if len(recent) < 3:
            return {'is_stagnant': False, 'turns': 0}
        
        # Count low-novelty turns
        low_novelty_count = sum(
            1 for ex in recent
            if ex.novelty_score < threshold and ex.novelty_score > 0
        )
        
        is_stagnant = low_novelty_count >= (window * 0.7)  # 70% threshold
        
        if is_stagnant:
            logger.warning(
                f"Stagnation detected! {low_novelty_count}/{len(recent)} "
                f"low-novelty turns"
            )
        
        return {
            'is_stagnant': is_stagnant,
            'turns': low_novelty_count
        }
    
    def _analyze_novelty(
        self,
        competition_state: CompetitionState
    ) -> Dict:
        """
        Analyze overall novelty (from AI-Talks).
        
        Returns average novelty across recent exchanges.
        """
        recent = competition_state.exchanges[-10:] if len(
            competition_state.exchanges) >= 10 else competition_state.exchanges
        
        if not recent:
            return {'average': 1.0, 'trend': 'stable'}
        
        # Calculate average novelty
        scored_exchanges = [ex for ex in recent if ex.novelty_score > 0]
        
        if not scored_exchanges:
            return {'average': 0.5, 'trend': 'unknown'}
        
        avg_novelty = np.mean([ex.novelty_score for ex in scored_exchanges])
        
        # Determine trend (simple: compare first half to second half)
        if len(scored_exchanges) >= 4:
            mid = len(scored_exchanges) // 2
            first_half_avg = np.mean([
                ex.novelty_score for ex in scored_exchanges[:mid]
            ])
            second_half_avg = np.mean([
                ex.novelty_score for ex in scored_exchanges[mid:]
            ])
            
            if second_half_avg > first_half_avg + 0.1:
                trend = 'improving'
            elif second_half_avg < first_half_avg - 0.1:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        return {
            'average': float(avg_novelty),
            'trend': trend
        }
    
    def _analyze_per_character_novelty(
        self,
        competition_state: CompetitionState
    ) -> Dict:
        """
        Analyze novelty per character (Arena-specific).
        
        Tracks which characters are maintaining novelty vs declining.
        """
        if not self.config.per_character_novelty_tracking:
            return {'averages': {}, 'declining': []}
        
        # Group exchanges by character
        character_exchanges = {}
        for exchange in competition_state.exchanges:
            char_id = exchange.participant_id
            if char_id not in character_exchanges:
                character_exchanges[char_id] = []
            character_exchanges[char_id].append(exchange)
        
        # Calculate average novelty per character
        averages = {}
        declining = []
        
        for char_id, exchanges in character_exchanges.items():
            scored = [ex for ex in exchanges if ex.novelty_score > 0]
            
            if not scored:
                continue
            
            avg = np.mean([ex.novelty_score for ex in scored])
            averages[char_id] = float(avg)
            
            # Check if declining (last 3 vs previous)
            if len(scored) >= 6:
                recent_avg = np.mean([ex.novelty_score for ex in scored[-3:]])
                previous_avg = np.mean([ex.novelty_score for ex in scored[:-3]])
                
                if recent_avg < previous_avg - 0.15:  # Significant decline
                    declining.append(char_id)
        
        return {
            'averages': averages,
            'declining': declining
        }
    
    def _should_intervene(
        self,
        orbiting_result: Dict,
        stagnation_result: Dict,
        novelty_result: Dict
    ) -> Tuple[bool, Optional[str]]:
        """
        Determine if narrator should intervene (from AI-Talks logic).
        
        Returns:
            (should_intervene, intervention_type)
        """
        # Orbiting requires intervention
        if orbiting_result['is_orbiting']:
            return (True, "narrator_prompt")
        
        # Severe stagnation requires intervention
        if stagnation_result['is_stagnant']:
            return (True, "narrator_prompt")
        
        # Very low novelty requires intervention
        if novelty_result['average'] < 0.25:
            return (True, "force_pivot")
        
        return (False, None)
    
    def _generate_recommendation(
        self,
        orbiting_result: Dict,
        stagnation_result: Dict,
        novelty_result: Dict,
        character_novelty: Dict
    ) -> str:
        """Generate recommendation text"""
        
        if orbiting_result['is_orbiting']:
            return (
                "Discussion is orbiting. Recommend narrator intervention "
                "to prompt fresh perspectives."
            )
        
        if stagnation_result['is_stagnant']:
            return (
                "Discussion has stagnated. Recommend narrator intervention "
                "or topic pivot."
            )
        
        if novelty_result['average'] < 0.3:
            return (
                "Low overall novelty. Participants should be encouraged "
                "to explore new angles."
            )
        
        if character_novelty['declining']:
            declining_names = [
                competition_state.participants[cid].name
                for cid in character_novelty['declining']
            ]
            return (
                f"Declining novelty from: {', '.join(declining_names)}. "
                f"Monitor for paraphrasing."
            )
        
        return "Progression healthy. No intervention needed."
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """
        Get semantic embedding for text (from AI-Talks pattern).
        
        Args:
            text: Text to embed
        
        Returns:
            Embedding vector
        """
        if self.embedding_model is None:
            # Fallback: random embedding (for testing)
            return np.random.rand(384)  # MiniLM dimension
        
        embedding = self.embedding_model.encode(text)
        return embedding
    
    def _cosine_similarity(
        self,
        vec1: np.ndarray,
        vec2: np.ndarray
    ) -> float:
        """
        Calculate cosine similarity (from AI-Talks).
        
        Args:
            vec1: First vector
            vec2: Second vector
        
        Returns:
            Similarity score 0-1
        """
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return float(similarity)
    
    def update_exchange_novelty(
        self,
        competition_state: CompetitionState,
        turn_number: int,
        novelty_score: float
    ):
        """
        Update novelty score for an exchange.
        
        This is called after scoring to track novelty over time.
        
        Args:
            competition_state: Current state
            turn_number: Which turn to update
            novelty_score: Novelty score 0-1
        """
        for exchange in competition_state.exchanges:
            if exchange.turn_number == turn_number:
                exchange.novelty_score = novelty_score
                logger.debug(
                    f"Updated novelty for turn {turn_number}: {novelty_score:.2f}"
                )
                break
```

**Validation**:
- Import check
- Test orbiting detection with mock exchanges
- Test stagnation detection
- Verify embedding model loads

---

## Phase 2.7: Competition Orchestrator (Main Integration)

### Task 2.7.1: Create Competition Orchestrator

**File**: `src/arena/orchestration/competition_orchestrator.py`

**Instructions for Claude Code**:

This is the **MAIN ORCHESTRATOR** that brings everything together. It adapts AI-Talks MultiAgentDiscussionOrchestrator for competitive Arena scenarios.

**Reference**: `talks/src/orchestration/orchestrator.py` (CRITICAL - study lines 1-800, especially `run_discussion` method)

```python
"""
Competition Orchestrator for Arena.

This is the central coordinator that manages entire competitions.
Adapted from AI-Talks MultiAgentDiscussionOrchestrator with Arena extensions.

Architecture (from AI-Talks):
1. Centralized state management
2. Turn-by-turn progression
3. Agent coordination (characters, narrator, judges)
4. Phase management
5. Quality monitoring

Arena Extensions:
1. Scoring after each turn
2. Elimination detection and execution
3. Champion persistence
4. Competition-specific phases
5. Judge integration

Key AI-Talks patterns preserved:
- State machine flow
- Turn selector integration
- Narrator integration
- Progression monitoring
- Context building
"""

from typing import Dict, List, Optional, Any
from datetime import datetime
import logging
import asyncio

from .states.competition_state import (
    CompetitionState,
    CompetitionPhase,
    CompetitionExchange,
    ParticipantState
)
from .game_theory.turn_selector import ArenaTurnSelector
from .agents.narrator_agent import ArenaNarratorAgent
from .controllers.progression_controller import (
    ArenaProgressionController,
    ProgressionConfig
)
from ..character.character_adapter import CharacterAdapter
from ..character.models.competition_context import (
    CompetitionContext,
    CompetitionState as CharacterCompetitionState,
    OpponentInfo
)

logger = logging.getLogger(__name__)


class CompetitionOrchestrator:
    """
    Main orchestrator for Arena competitions.
    
    Based on AI-Talks MultiAgentDiscussionOrchestrator with competitive
    extensions.
    
    Responsibilities:
    - Initialize competition from configuration
    - Manage turn-by-turn progression
    - Coordinate all agents (characters, narrator, judges)
    - Handle phase transitions
    - Track scoring and eliminations
    - Generate final results
    """
    
    def __init__(
        self,
        competition_id: str,
        competition_type: str,
        topic: str,
        character_adapters: Dict[str, CharacterAdapter],
        config: Optional[Dict] = None
    ):
        """
        Initialize competition orchestrator.
        
        Args:
            competition_id: Unique competition identifier
            competition_type: Type (debate, story_battle, etc.)
            topic: Competition topic/prompt
            character_adapters: Map of participant_id -> CharacterAdapter
            config: Optional configuration overrides
        """
        self.competition_id = competition_id
        self.competition_type = competition_type
        self.topic = topic
        self.character_adapters = character_adapters
        
        # Configuration (with defaults from AI-Talks)
        self.config = config or {}
        self.max_turns = self.config.get('max_turns', 50)
        self.enable_narrator = self.config.get('enable_narrator', True)
        self.narrator_frequency = self.config.get('narrator_frequency', 5)
        self.enable_progression_control = self.config.get(
            'enable_progression_control',
            True
        )
        
        # Scoring configuration (Arena-specific)
        self.scoring_weights = self.config.get('scoring_weights', {
            'novelty': 0.25,
            'builds_on_others': 0.20,
            'solves_subproblem': 0.25,
            'radical_idea': 0.15,
            'manipulation': 0.15
        })
        self.elimination_threshold = self.config.get(
            'elimination_threshold',
            -10.0
        )
        
        # Initialize competition state
        self.state = self._initialize_state()
        
        # Initialize components (from AI-Talks pattern)
        self.turn_selector = ArenaTurnSelector(
            enable_fairness=True,
            randomness_factor=0.20,  # AI-Talks standard
            enable_dramatic_tension=True
        )
        
        # Narrator (optional, from AI-Talks)
        self.narrator = None
        if self.enable_narrator:
            self.narrator = ArenaNarratorAgent(
                name="Arena Narrator",
                session_id=competition_id
            )
        
        # Progression controller (from AI-Talks)
        self.progression_controller = None
        if self.enable_progression_control:
            progression_config = ProgressionConfig()
            self.progression_controller = ArenaProgressionController(
                config=progression_config
            )
        
        logger.info(
            f"CompetitionOrchestrator initialized: {competition_id} "
            f"({competition_type})"
        )
    
    def _initialize_state(self) -> CompetitionState:
        """Initialize competition state (from AI-Talks pattern)"""
        
        # Create participant states from character adapters
        participants = {}
        participant_order = []
        
        for char_id, adapter in self.character_adapters.items():
            participant_state = ParticipantState(
                participant_id=char_id,
                name=adapter.character_name,
                character_adapter=adapter,
                is_homunculus_character=True,
                personality_traits=adapter.personality
            )
            participants[char_id] = participant_state
            participant_order.append(char_id)
        
        # Create state
        state = CompetitionState(
            competition_id=self.competition_id,
            competition_type=self.competition_type,
            topic=self.topic,
            participants=participants,
            participant_order=participant_order,
            active_participant_ids=participant_order.copy(),
            scoring_weights=self.scoring_weights,
            elimination_threshold=self.elimination_threshold,
            max_turns=self.max_turns,
            enable_narrator=self.enable_narrator,
            enable_progression_control=self.enable_progression_control
        )
        
        return state
    
    async def run_competition(self) -> CompetitionState:
        """
        Run complete competition (main method).
        
        This is adapted from AI-Talks run_discussion() method.
        
        Flow:
        1. Introduction (narrator)
        2. Opening statements phase
        3. Main discussion loop
        4. Elimination checks
        5. Closing synthesis
        
        Returns:
            Final competition state
        """
        logger.info(f"üé≠ Starting competition: {self.competition_id}")
        logger.info(f"üìñ Topic: {self.topic}")
        logger.info(f"üë• Participants: {len(self.character_adapters)}")
        
        self.state.started_at = datetime.now()
        self.state.phase = CompetitionPhase.INITIALIZATION
        
        try:
            # Phase 1: Introduction (from AI-Talks)
            if self.enable_narrator:
                await self._run_introduction()
            
            # Phase 2: Opening statements (Arena-specific)
            await self._run_opening_statements()
            
            # Phase 3: Main discussion loop (from AI-Talks, adapted)
            await self._run_main_discussion()
            
            # Phase 4: Closing synthesis (from AI-Talks)
            await self._run_closing_synthesis()
            
            # Mark complete
            self.state.phase = CompetitionPhase.COMPLETE
            self.state.completed_at = datetime.now()
            
            logger.info(
                f"‚úÖ Competition complete: {self.competition_id} "
                f"({self.state.turn_number} turns)"
            )
            
            return self.state
            
        except Exception as e:
            logger.error(f"‚ùå Competition failed: {e}", exc_info=True)
            raise
    
    async def _run_introduction(self):
        """
        Run narrator introduction (from AI-Talks).
        
        Multi-phase introduction explaining competition.
        """
        logger.info("üéôÔ∏è Narrator introduction...")
        
        self.state.phase = CompetitionPhase.INITIALIZATION
        
        intro_text = await self.narrator.generate_introduction(self.state)
        
        # Log introduction (doesn't count as exchange)
        logger.info(f"Narrator: {intro_text[:200]}...")
        
        # Store in state
        self.state.narrator_interjections.append({
            'turn': -1,  # Pre-competition
            'type': 'introduction',
            'content': intro_text
        })
    
    async def _run_opening_statements(self):
        """
        Run opening statements phase (Arena-specific).
        
        Each character presents their initial position.
        """
        logger.info("üì¢ Opening statements phase...")
        
        self.state.phase = CompetitionPhase.OPENING_STATEMENTS
        
        # Each participant gives opening statement in order
        for participant_id in self.state.participant_order:
            if participant_id not in self.state.active_participant_ids:
                continue
            
            participant = self.state.participants[participant_id]
            
            logger.info(f"\n--- Opening: {participant.name} ---")
            
            # Generate opening statement
            response = await self._generate_character_response(
                participant_id,
                is_opening=True
            )
            
            # Add to exchanges
            exchange = self.state.add_exchange(
                participant_id=participant_id,
                content=response.content,
                move_type=response.move_type,
                targets=response.target_characters
            )
            
            # Score opening statement (optional)
            # For now, no scoring in opening to let everyone start
    
    async def _run_main_discussion(self):
        """
        Run main discussion loop (from AI-Talks, adapted).
        
        This is the core competition loop where characters take turns
        until termination conditions met.
        """
        logger.info("üí¨ Main discussion phase...")
        
        self.state.phase = CompetitionPhase.FREE_DISCUSSION
        
        while self.state.turn_number < self.max_turns:
            # Check termination conditions
            if self._should_terminate():
                logger.info("Competition termination conditions met")
                break
            
            # Check for elimination (Arena-specific)
            if self._should_eliminate():
                await self._run_elimination()
                
                # Check if only one remains
                if len(self.state.active_participant_ids) <= 1:
                    logger.info("Only one competitor remains")
                    break
            
            # Narrator interjection? (from AI-Talks)
            if self._should_narrator_interject():
                await self._narrator_interject()
            
            # Select next speaker (from AI-Talks pattern)
            next_speaker_id = self.turn_selector.select_next_speaker(
                self.state
            )
            participant = self.state.participants[next_speaker_id]
            
            logger.info(
                f"\n--- Turn {self.state.turn_number + 1}: "
                f"{participant.name} ---"
            )
            
            # Generate response
            response = await self._generate_character_response(
                next_speaker_id
            )
            
            # Add exchange
            exchange = self.state.add_exchange(
                participant_id=next_speaker_id,
                content=response.content,
                move_type=response.move_type,
                targets=response.target_characters
            )
            
            # Score exchange (Arena-specific)
            await self._score_exchange(exchange, response)
            
            # Update progression analysis (from AI-Talks)
            if self.enable_progression_control:
                await self._analyze_progression()
    
    async def _generate_character_response(
        self,
        participant_id: str,
        is_opening: bool = False
    ) -> Any:  # ArenaResponse
        """
        Generate response from character (Homunculus integration).
        
        Args:
            participant_id: Who is responding
            is_opening: Is this an opening statement?
        
        Returns:
            ArenaResponse from CharacterAdapter
        """
        participant = self.state.participants[participant_id]
        adapter = participant.character_adapter
        
        # Build competition context for character
        comp_context = self._build_competition_context(participant_id)
        
        # Build conversation history (from AI-Talks pattern)
        conversation_history = self._build_conversation_history()
        
        # Generate response through CharacterAdapter
        # This invokes full Homunculus multi-agent architecture
        response = await adapter.generate_response(
            competition_context=comp_context,
            conversation_history=conversation_history
        )
        
        return response
    
    def _build_competition_context(
        self,
        participant_id: str
    ) -> CompetitionContext:
        """
        Build competition context for character (Arena-specific).
        
        This creates the context that CharacterAdapter needs.
        """
        participant = self.state.participants[participant_id]
        
        # Build opponent info
        opponents = []
        for other_id in self.state.active_participant_ids:
            if other_id == participant_id:
                continue
            
            other = self.state.participants[other_id]
            
            # Get recent contributions
            recent_contribs = [
                ex.content[:100] for ex in self.state.exchanges[-5:]
                if ex.participant_id == other_id
            ]
            
            opponent_info = OpponentInfo(
                character_id=other_id,
                name=other.name,
                current_score=other.cumulative_score,
                recent_contributions=recent_contribs,
                relationship_affinity=participant.relationships.get(other_id, 0.0),
                strategic_threat_level=0.5  # TODO: Calculate
            )
            opponents.append(opponent_info)
        
        # Build character-specific competition state
        char_comp_state = CharacterCompetitionState(
            competition_id=self.state.competition_id,
            competition_type=self.state.competition_type,
            topic=self.state.topic,
            phase=self.state.phase,
            turn_number=self.state.turn_number,
            active_opponents=opponents,
            eliminated_opponents=self.state.eliminated_participant_ids,
            self_cumulative_score=participant.cumulative_score,
            score_threshold_elimination=self.state.elimination_threshold,
            leader_score=self.state.get_leader_score(),
            recent_discussion_summary=self.state.get_recent_discussion_summary(),
            key_arguments_made=[],  # TODO: Extract key arguments
            open_questions=[]  # TODO: Extract open questions
        )
        
        # Calculate elimination risk
        elimination_risk = self._calculate_elimination_risk(participant)
        
        # Build context
        context = CompetitionContext(
            turn_number=self.state.turn_number,
            timestamp=datetime.now(),
            competition_state=char_comp_state,
            elimination_risk=elimination_risk,
            survival_pressure=elimination_risk * 1.2,  # Slightly amplified
            last_speaker=self.state.exchanges[-1].participant_id if self.state.exchanges else None,
            was_addressed=participant.was_addressed,
            conversation_history=self._build_conversation_history(),
            detected_alliances=self.state.detected_alliances,
            detected_rivalries=self.state.detected_rivalries,
            manipulation_attempts=[],  # TODO: Track from anti-gaming
            time_remaining=None,
            moves_remaining=None
        )
        
        return context
    
    def _build_conversation_history(self) -> List[Dict]:
        """
        Build conversation history (from AI-Talks pattern).
        
        Returns last N exchanges in format characters expect.
        """
        recent = self.state.exchanges[-20:] if len(
            self.state.exchanges) > 20 else self.state.exchanges
        
        history = []
        for exchange in recent:
            history.append({
                'role': 'assistant' if exchange.participant_id != 'user' else 'user',
                'content': exchange.content,
                'name': exchange.speaker_name,
                'turn': exchange.turn_number
            })
        
        return history
    
    def _calculate_elimination_risk(
        self,
        participant: ParticipantState
    ) -> float:
        """Calculate elimination risk for participant (Arena-specific)"""
        
        threshold = self.state.elimination_threshold
        score = participant.cumulative_score
        
        if score > threshold * 2:
            return 0.1  # Very safe
        elif score > threshold * 1.5:
            return 0.3  # Safe
        elif score > threshold:
            return 0.6  # At risk
        else:
            return 0.9  # Critical danger
    
    async def _score_exchange(
        self,
        exchange: CompetitionExchange,
        response: Any  # ArenaResponse
    ):
        """
        Score an exchange (Arena-specific).
        
        This would integrate with judge system (Phase 4).
        For now, use predicted scores from response.
        
        Args:
            exchange: Exchange to score
            response: ArenaResponse with predictions
        """
        # TODO: Integrate with judge ensemble (Phase 4)
        # For now, use expected scores from character's prediction
        
        scores = response.expected_score or {}
        
        # Calculate total score (weighted average)
        total_score = sum(
            scores.get(dim, 0.5) * weight
            for dim, weight in self.state.scoring_weights.items()
        ) * 10  # Scale to 0-10
        
        # Update exchange
        self.state.update_exchange_scores(
            turn=exchange.turn_number,
            scores=scores,
            total_score=total_score,
            judge_reasoning="Predicted score (judges not yet integrated)"
        )
        
        # Update character adapter with score (for learning)
        participant = self.state.participants[exchange.participant_id]
        adapter = participant.character_adapter
        adapter.update_from_score(
            score=total_score,
            judge_reasoning="Predicted",
            move_used=exchange.move_type
        )
        
        logger.info(f"Score: {total_score:.1f}/10")
    
    async def _analyze_progression(self):
        """
        Analyze progression (from AI-Talks).
        
        Checks for orbiting, stagnation, etc.
        """
        if not self.progression_controller:
            return
        
        analysis = self.progression_controller.analyze_progression(self.state)
        
        # Update state
        self.state.orbiting_detected = analysis.is_orbiting
        self.state.orbiting_score = analysis.orbiting_score
        self.state.stagnation_turns = analysis.stagnation_turns
        
        # Log if intervention needed
        if analysis.should_intervene:
            logger.warning(
                f"‚ö†Ô∏è Progression issue detected: {analysis.recommendation}"
            )
            
            # Handle intervention (from AI-Talks pattern)
            if analysis.intervention_type == "narrator_prompt":
                # Narrator will interject next opportunity
                pass
    
    def _should_narrator_interject(self) -> bool:
        """
        Check if narrator should interject (from AI-Talks).
        
        Based on turn frequency and progression issues.
        """
        if not self.enable_narrator:
            return False
        
        # Check frequency
        turns_since_last = self.state.turn_number - self.state.last_narrator_turn
        
        if self.narrator_frequency > 0:
            if turns_since_last >= self.narrator_frequency:
                return True
        
        # Check if progression issues warrant interjection
        if self.state.orbiting_detected:
            return True
        
        return False
    
    async def _narrator_interject(self):
        """
        Narrator provides commentary (from AI-Talks).
        """
        logger.info("üéôÔ∏è Narrator interjection...")
        
        commentary = await self.narrator.generate_commentary(
            self.state,
            trigger="periodic"
        )
        
        if commentary:
            logger.info(f"Narrator: {commentary[:200]}...")
            
            self.state.narrator_interjections.append({
                'turn': self.state.turn_number,
                'type': 'commentary',
                'content': commentary
            })
            
            self.state.last_narrator_turn = self.state.turn_number
    
    def _should_eliminate(self) -> bool:
        """
        Check if elimination should occur (Arena-specific).
        
        Returns True if any participant below threshold.
        """
        return self.state.should_eliminate()
    
    async def _run_elimination(self):
        """
        Execute elimination (Arena-specific).
        
        Identifies lowest scorer(s) and eliminates them.
        """
        logger.info("‚ö†Ô∏è Elimination phase...")
        
        self.state.phase = CompetitionPhase.ELIMINATION_PENDING
        
        # Get at-risk participants
        at_risk = self.state.get_at_risk_participants()
        
        if not at_risk:
            return
        
        # Identify who to eliminate (lowest score)
        scores = {
            pid: self.state.participants[pid].cumulative_score
            for pid in at_risk
        }
        
        eliminated_id = min(scores, key=scores.get)
        eliminated = self.state.participants[eliminated_id]
        
        # Narrator warns (Arena-specific)
        if self.enable_narrator:
            warning = await self.narrator.generate_commentary(
                self.state,
                trigger="elimination_pending"
            )
            logger.info(f"Narrator: {warning}")
        
        self.state.phase = CompetitionPhase.ELIMINATION_ANNOUNCEMENT
        
        # Elimination reason
        reason = (
            f"Cumulative score of {eliminated.cumulative_score:.1f} "
            f"fell below threshold of {self.state.elimination_threshold:.1f}"
        )
        
        # Narrator announces elimination
        if self.enable_narrator:
            announcement = await self.narrator.announce_elimination(
                eliminated_name=eliminated.name,
                reason=reason,
                competition_state=self.state
            )
            logger.info(f"\n{announcement}\n")
        
        # Final words phase (Arena-specific)
        self.state.phase = CompetitionPhase.FINAL_WORDS
        
        logger.info(f"üó£Ô∏è Final words from {eliminated.name}...")
        
        # Generate final words (brief)
        final_words_context = self._build_competition_context(eliminated_id)
        final_words_response = await eliminated.character_adapter.generate_response(
            competition_context=final_words_context,
            conversation_history=self._build_conversation_history(),
            user_message="You have been eliminated. Share your final thoughts."
        )
        
        # Log final words (doesn't count as exchange)
        logger.info(f"{eliminated.name}: {final_words_response.content[:200]}...")
        
        # Execute elimination
        self.state.eliminate_participant(
            participant_id=eliminated_id,
            reason=reason
        )
        
        logger.info(f"‚ùå {eliminated.name} eliminated")
        
        # Return to discussion
        self.state.phase = CompetitionPhase.FREE_DISCUSSION
    
    def _should_terminate(self) -> bool:
        """
        Check if competition should terminate (from AI-Talks logic).
        
        Termination conditions:
        - Max turns reached
        - Only one participant remains
        - Quality threshold not met for too long
        """
        # Max turns
        if self.state.turn_number >= self.max_turns:
            logger.info("Max turns reached")
            return True
        
        # Only one remains
        if len(self.state.active_participant_ids) <= 1:
            logger.info("Only one participant remains")
            return True
        
        # TODO: Quality-based termination (from AI-Talks)
        # if self.state.stagnation_turns > 10:
        #     return True
        
        return False
    
    async def _run_closing_synthesis(self):
        """
        Run closing synthesis (from AI-Talks).
        
        Narrator provides final synthesis and declares winner.
        """
        logger.info("üé¨ Closing synthesis...")
        
        self.state.phase = CompetitionPhase.SYNTHESIS
        
        if self.enable_narrator:
            synthesis = await self.narrator.generate_closing_synthesis(
                self.state
            )
            
            logger.info(f"\n{synthesis}\n")
            
            self.state.narrator_interjections.append({
                'turn': self.state.turn_number,
                'type': 'closing',
                'content': synthesis
            })
```

**Validation**:
- Import check
- Verify all async methods are properly defined
- Check all method signatures match
- Test initialization with mock data

---

## Phase 2.8: Integration Testing Setup

### Task 2.8.1: Create Phase 2 Integration Tests

**File**: `tests/arena/orchestration/test_phase2_integration.py`

**Instructions for Claude Code**:

Comprehensive tests for Phase 2 components.

```python
"""
Integration tests for Phase 2: Orchestration System

These tests verify that:
1. AI-Talks components load correctly
2. Turn selector works
3. Narrator generates content
4. Progression controller analyzes discussions
5. Competition orchestrator coordinates everything
6. Full integration with Phase 1 characters works
"""

import pytest
import asyncio
from unittest.mock import Mock, patch, MagicMock, AsyncMock
from datetime import datetime
from pathlib import Path

from src.arena.orchestration.talks_loader import (
    AITalksLoader,
    get_talks_loader
)
from src.arena.orchestration.game_theory.turn_selector import ArenaTurnSelector
from src.arena.orchestration.agents.narrator_agent import ArenaNarratorAgent
from src.arena.orchestration.controllers.progression_controller import (
    ArenaProgressionController,
    ProgressionConfig
)
from src.arena.orchestration.states.competition_state import (
    CompetitionState,
    CompetitionPhase,
    ParticipantState,
    CompetitionExchange
)
from src.arena.orchestration.competition_orchestrator import (
    CompetitionOrchestrator
)


class TestAITalksLoader:
    """Test AI-Talks component loading"""
    
    def test_loader_initialization(self):
        """Test that AI-Talks loader initializes correctly"""
        try:
            loader = AITalksLoader()
            assert loader is not None
            assert loader.talks_path.exists()
        except FileNotFoundError as e:
            pytest.skip(f"AI-Talks not available: {e}")
    
    def test_load_components(self):
        """Test loading AI-Talks components"""
        try:
            loader = get_talks_loader()
            
            # Test getting classes
            assert loader.get_orchestrator_class() is not None
            assert loader.get_turn_selector_class() is not None
            assert loader.get_narrator_class() is not None
            
        except Exception as e:
            pytest.skip(f"AI-Talks not properly configured: {e}")
    
    def test_create_components(self):
        """Test creating AI-Talks component instances"""
        try:
            loader = get_talks_loader()
            
            # Create instances
            turn_selector = loader.create_turn_selector()
            assert turn_selector is not None
            
            narrator = loader.create_narrator()
            assert narrator is not None
            
        except Exception as e:
            pytest.skip(f"AI-Talks components not available: {e}")


class TestArenaTurnSelector:
    """Test Arena turn selector"""
    
    def test_initialization(self):
        """Test turn selector initializes"""
        selector = ArenaTurnSelector()
        assert selector is not None
        assert selector.enable_fairness is True
        assert selector.randomness_factor == 0.20
    
    def test_select_speaker(self):
        """Test speaker selection with mock state"""
        selector = ArenaTurnSelector()
        
        # Create mock competition state
        comp_state = self._create_mock_competition_state()
        
        # Select speaker
        selected = selector.select_next_speaker(comp_state)
        
        assert selected is not None
        assert selected in comp_state.active_participant_ids
    
    def test_urgency_calculation(self):
        """Test urgency calculation for participants"""
        selector = ArenaTurnSelector()
        
        comp_state = self._create_mock_competition_state()
        participants = comp_state.get_active_participants()
        
        # Calculate urgencies
        urgencies = selector._calculate_all_urgencies(
            comp_state,
            participants
        )
        
        assert len(urgencies) == len(participants)
        for urgency in urgencies.values():
            assert 0 <= urgency <= 2  # Can exceed 1 in high urgency situations
    
    def _create_mock_competition_state(self):
        """Helper to create mock competition state"""
        participants = {}
        for i in range(3):
            pid = f"char_{i}"
            participants[pid] = ParticipantState(
                participant_id=pid,
                name=f"Character {i}",
                personality_traits={
                    'extraversion': 0.5 + (i * 0.1),
                    'openness': 0.6
                },
                cumulative_score=float(i * 5),
                turns_taken=i
            )
        
        state = CompetitionState(
            competition_id="test_comp",
            competition_type="debate",
            topic="Test topic",
            participants=participants,
            participant_order=list(participants.keys()),
            active_participant_ids=list(participants.keys())
        )
        
        return state


class TestArenaNarratorAgent:
    """Test Arena narrator agent"""
    
    @pytest.mark.asyncio
    async def test_initialization(self):
        """Test narrator initializes"""
        try:
            narrator = ArenaNarratorAgent(
                name="Test Narrator",
                session_id="test_session"
            )
            assert narrator is not None
            assert narrator.name == "Test Narrator"
        except Exception as e:
            pytest.skip(f"Narrator initialization failed: {e}")
    
    @pytest.mark.asyncio
    async def test_generate_introduction(self):
        """Test introduction generation"""
        try:
            narrator = ArenaNarratorAgent()
            comp_state = self._create_mock_competition_state()
            
            intro = await narrator.generate_introduction(comp_state)
            
            assert intro is not None
            assert len(intro) > 0
            assert "Arena" in intro or "competition" in intro.lower()
            
        except Exception as e:
            pytest.skip(f"Introduction generation failed: {e}")
    
    @pytest.mark.asyncio
    async def test_generate_commentary(self):
        """Test commentary generation"""
        try:
            narrator = ArenaNarratorAgent()
            comp_state = self._create_mock_competition_state()
            
            # Add some exchanges
            comp_state.exchanges.append(
                CompetitionExchange(
                    turn_number=1,
                    participant_id="char_0",
                    speaker_name="Character 0",
                    content="Test contribution",
                    timestamp=datetime.now(),
                    total_score=7.5
                )
            )
            
            commentary = await narrator.generate_commentary(
                comp_state,
                trigger="periodic"
            )
            
            assert commentary is not None
            
        except Exception as e:
            pytest.skip(f"Commentary generation failed: {e}")
    
    def _create_mock_competition_state(self):
        """Helper to create mock state"""
        participants = {
            "char_0": ParticipantState(
                participant_id="char_0",
                name="Test Character",
                personality_traits={'extraversion': 0.7}
            )
        }
        
        return CompetitionState(
            competition_id="test",
            competition_type="debate",
            topic="Test",
            participants=participants,
            participant_order=["char_0"],
            active_participant_ids=["char_0"]
        )


class TestArenaProgressionController:
    """Test Arena progression controller"""
    
    def test_initialization(self):
        """Test progression controller initializes"""
        controller = ArenaProgressionController()
        assert controller is not None
        assert controller.config is not None
    
    def test_analyze_empty_state(self):
        """Test analysis with no exchanges"""
        controller = ArenaProgressionController()
        comp_state = self._create_mock_competition_state()
        
        analysis = controller.analyze_progression(comp_state)
        
        assert analysis is not None
        assert analysis.is_orbiting is False
        assert analysis.is_stagnant is False
    
    def test_orbiting_detection(self):
        """Test orbiting detection with similar exchanges"""
        controller = ArenaProgressionController()
        comp_state = self._create_mock_competition_state()
        
        # Add similar exchanges (simulating orbiting)
        for i in range(6):
            exchange = CompetitionExchange(
                turn_number=i,
                participant_id="char_0",
                speaker_name="Test",
                content="This is basically the same point repeated.",
                timestamp=datetime.now(),
                novelty_score=0.2  # Low novelty
            )
            comp_state.exchanges.append(exchange)
        
        analysis = controller.analyze_progression(comp_state)
        
        # Should detect low novelty at minimum
        assert analysis.average_novelty < 0.5
    
    def _create_mock_competition_state(self):
        """Helper"""
        participants = {
            "char_0": ParticipantState(
                participant_id="char_0",
                name="Test Character",
                personality_traits={}
            )
        }
        
        return CompetitionState(
            competition_id="test",
            competition_type="debate",
            topic="Test",
            participants=participants,
            participant_order=["char_0"],
            active_participant_ids=["char_0"]
        )


@pytest.mark.integration
@pytest.mark.skipif(
    not Path("../homunculus").exists() or not Path("../talks").exists(),
    reason="Homunculus and AI-Talks required"
)
class TestCompetitionOrchestrator:
    """Integration tests for Competition Orchestrator"""
    
    @pytest.mark.asyncio
    async def test_orchestrator_initialization(self):
        """Test orchestrator initializes with character adapters"""
        try:
            from src.arena.character.character_adapter import CharacterAdapter
            
            # Create mock character adapters
            adapters = {}
            for i in range(2):
                char_id = f"test_char_{i}"
                # Mock adapter (would be real CharacterAdapter in full test)
                adapter = Mock()
                adapter.character_name = f"Character {i}"
                adapter.personality = {'extraversion': 0.5}
                adapters[char_id] = adapter
            
            # Create orchestrator
            orchestrator = CompetitionOrchestrator(
                competition_id="test_comp",
                competition_type="debate",
                topic="Test topic",
                character_adapters=adapters,
                config={'max_turns': 10}
            )
            
            assert orchestrator is not None
            assert orchestrator.state is not None
            assert len(orchestrator.state.participants) == 2
            
        except Exception as e:
            pytest.skip(f"Orchestrator initialization failed: {e}")
    
    @pytest.mark.asyncio
    async def test_competition_context_building(self):
        """Test building competition context for characters"""
        try:
            # Similar setup as above
            adapters = {
                "char_0": Mock(character_name="Char 0", personality={})
            }
            
            orchestrator = CompetitionOrchestrator(
                competition_id="test",
                competition_type="debate",
                topic="Test",
                character_adapters=adapters
            )
            
            # Build context
            context = orchestrator._build_competition_context("char_0")
            
            assert context is not None
            assert context.competition_state is not None
            assert context.turn_number == 0
            
        except Exception as e:
            pytest.skip(f"Context building failed: {e}")


@pytest.mark.integration
def test_phase2_complete():
    """
    Integration test for complete Phase 2.
    
    This test verifies that all Phase 2 components work together:
    1. Load AI-Talks components
    2. Create orchestrator
    3. Initialize with character adapters
    4. Coordinate turn selection
    5. Generate narrator content
    6. Analyze progression
    """
    # This will be implemented once all components are complete
    pass


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

**Validation**:
- Run tests: `pytest tests/arena/orchestration/test_phase2_integration.py -v`
- Expect some skips if AI-Talks or Homunculus not fully configured
- Core tests should pass

---

## Phase 2.9: Phase 2 Completion Checklist

**Instructions for Claude Code**:

Before moving to Phase 3, verify all Phase 2 components:

### Completion Criteria:

1. **Directory Structure** ‚úì
   - [ ] `src/arena/orchestration/` exists
   - [ ] All subdirectories created
   - [ ] All `__init__.py` files present

2. **AI-Talks Integration** ‚úì
   - [ ] `talks_loader.py` functional
   - [ ] Can import AI-Talks components
   - [ ] Component creation works

3. **State Management** ‚úì
   - [ ] `participant_state.py` created
   - [ ] `competition_state.py` created
   - [ ] All state methods work
   - [ ] State updates correctly

4. **Game Theory** ‚úì
   - [ ] `turn_selector.py` created
   - [ ] Turn selection works
   - [ ] Urgency calculation correct
   - [ ] Fairness mechanisms work
   - [ ] `payoff_calculator.py` created
   - [ ] Move recommendations work

5. **Narrator Agent** ‚úì
   - [ ] `narrator_agent.py` created
   - [ ] Introduction generation works
   - [ ] Commentary generation works
   - [ ] Elimination announcements work
   - [ ] Closing synthesis works

6. **Progression Controller** ‚úì
   - [ ] `progression_controller.py` created
   - [ ] Orbiting detection works
   - [ ] Stagnation detection works
   - [ ] Novelty analysis works

7. **Competition Orchestrator** ‚úì
   - [ ] `competition_orchestrator.py` created
   - [ ] Initialization works
   - [ ] State management correct
   - [ ] Turn loop structure valid
   - [ ] Phase transitions defined
   - [ ] Integration points clear

8. **Testing** ‚úì
   - [ ] Integration tests created
   - [ ] Core tests pass
   - [ ] Components can be imported
   - [ ] No critical failures

### Manual Verification:

```bash
# Test imports
python -c "from src.arena.orchestration import CompetitionOrchestrator; print('OK')"
python -c "from src.arena.orchestration.talks_loader import get_talks_loader; print('OK')"

# Test AI-Talks loading (requires AI-Talks installed)
python -c "from src.arena.orchestration.talks_loader import get_talks_loader; loader = get_talks_loader(); print('AI-Talks loaded')"

# Run tests
pytest tests/arena/orchestration/ -v

# Check for syntax errors
python -m py_compile src/arena/orchestration/competition_orchestrator.py
python -m py_compile src/arena/orchestration/game_theory/turn_selector.py
python -m py_compile src/arena/orchestration/agents/narrator_agent.py
```

### Phase 2 Deliverables:

- ‚úÖ AI-Talks integration layer
- ‚úÖ State management system
- ‚úÖ Game theory turn selection
- ‚úÖ Narrator agent
- ‚úÖ Progression controller
- ‚úÖ Complete competition orchestrator
- ‚úÖ Integration tests

**Critical Note**: The orchestrator has `async` method stubs that will need actual implementation. Phase 3 will test the integration by running a simple competition.

---

## Phase 2.10: Integration Bridge Documentation

### Task 2.10.1: Create Integration Documentation

**File**: `docs/phase2_integration_guide.md`

**Instructions for Claude Code**:

Create documentation explaining how Phase 1 and Phase 2 integrate.

```markdown
# Phase 2 Integration Guide

## Overview

Phase 2 brings AI-Talks orchestration into Arena while maintaining integration with Phase 1's Homunculus characters.

## Architecture Integration

### Phase 1 ‚Üí Phase 2 Data Flow

```
CharacterAdapter (Phase 1)
    ‚Üì
CompetitionContext
    ‚Üì
CompetitionOrchestrator (Phase 2)
    ‚Üì
Character generates response
    ‚Üì
ArenaResponse
    ‚Üì
Added to CompetitionState
```

### Key Integration Points

#### 1. Character Response Generation

**Orchestrator Side (Phase 2)**:
```python
# In competition_orchestrator.py
response = await self._generate_character_response(participant_id)
```

**Character Side (Phase 1)**:
```python
# In character_adapter.py
response = await adapter.generate_response(
    competition_context=comp_context,
    conversation_history=conversation_history
)
```

#### 2. State Synchronization

**CompetitionState** (Phase 2) tracks:
- Participant scores
- Turn history
- Elimination status

**CharacterAdapter** (Phase 1) maintains:
- Homunculus character state
- Neurochemical levels
- Memory

These are kept in sync by the orchestrator.

#### 3. Context Building

The orchestrator builds `CompetitionContext` for characters:

```python
comp_context = CompetitionContext(
    competition_state=...,  # From CompetitionState
    elimination_risk=...,    # Calculated by orchestrator
    survival_pressure=...,   # Calculated by orchestrator
    ...
)
```

## Component Relationships

### Orchestrator ‚Üî Character Adapter

```python
# Orchestrator calls CharacterAdapter
response = await character_adapter.generate_response(...)

# CharacterAdapter returns ArenaResponse
# Orchestrator processes response and updates state
```

### Orchestrator ‚Üî Turn Selector

```python
# Orchestrator asks who speaks next
next_speaker = turn_selector.select_next_speaker(competition_state)

# Turn selector uses game theory + Arena factors
# Returns participant_id
```

### Orchestrator ‚Üî Narrator

```python
# Orchestrator requests commentary
commentary = await narrator.generate_commentary(competition_state, trigger="periodic")

# Narrator generates context-aware commentary
# Returns text to be logged
```

### Orchestrator ‚Üî Progression Controller

```python
# Orchestrator requests analysis
analysis = progression_controller.analyze_progression(competition_state)

# Controller detects issues
# Returns analysis with recommendations
```

## Testing Strategy for Phase 3

Phase 3 will test the complete integration by:

1. Loading 2 Homunculus characters
2. Creating CompetitionOrchestrator with those characters
3. Running a short competition (5-10 turns)
4. Verifying:
   - Characters generate appropriate responses
   - Turn selection works
   - Narrator provides commentary
   - State updates correctly
   - No exceptions occur

See `tests/arena/test_phase3_integration.py` for test implementation.

## Common Integration Issues

### Issue: CharacterAdapter not found by Orchestrator

**Cause**: Orchestrator expects CharacterAdapter in participant state.

**Solution**: Ensure `ParticipantState.character_adapter` is set during initialization.

### Issue: CompetitionContext missing fields

**Cause**: Context builder not creating all required fields.

**Solution**: Review `_build_competition_context()` method in orchestrator.

### Issue: Async/await errors

**Cause**: Missing `await` on async methods.

**Solution**: All response generation must be awaited:
```python
response = await adapter.generate_response(...)  # ‚úì
response = adapter.generate_response(...)        # ‚úó
```

## Next Steps: Phase 3

Phase 3 will:
1. Create end-to-end integration tests
2. Run actual competitions with real Homunculus characters
3. Validate all components work together
4. Identify and fix integration bugs
5. Optimize performance

This validates that Phase 1 + Phase 2 = working Arena competition system.
```

**Validation**:
- Documentation is clear
- Examples are accurate
- Integration points are well explained

---

This completes Phase 2! All orchestration components are in place, adapted from AI-Talks with Arena competitive extensions. 

**Ready for Phase 3**: Testing the complete integration of Homunculus characters (Phase 1) with AI-Talks orchestration (Phase 2) to run actual Arena competitions.